\section{Design}

\subsection{High Level Overview}

The system will comprise 3 parts required to simulate and program a proprietary processor. It will include a virtual machine to emulate the execution of binary machine code catridges, an assembler to translate higher level assembly code into machine code, and finally a compiler for a higher level language to easily program complex applications to run on the processor.

The virtual machine consists of two main processes, the debugger and interpreter. The interpreter will continiously step through memory, decoding and executing instructions sequentially whilst displaying the contents of VRAM through the pixel display.

\bigskip

\shadowbox{
    \includegraphics[width=13cm]{Screenshot 2024-07-23 at 13.51.25.png}
}

\bigskip

\begin{multicols}{2}[\columnsep-0.5em] 
    The assembler consists of a single pipeline for transforming ASCII assembly programs into binary machine code. The files are loaded into the interpreter which stores their contents in a string. The contents are then tokenised by a lexer into a list of objects representing the foundational elements of the program (e.g. STRING, BRACE, NUMBER) and parsed into a sequence of assembly language instructions. These instructions are translated into binary machine code according to the instruction set architecture (defined in \ref{sec:MachineCodeEncoding}) which is then written to a file and stored on the users computer.


    \columnbreak

    \shadowbox{
        \includegraphics[width=5cm]{Screenshot 2024-07-23 at 12.47.14.png}
    }

\end{multicols}


\begin{wrapfigure}[13]{r}{8.5cm}
\shadowbox{
    \includegraphics[width=8cm]{Screenshot 2024-06-24 at 19.30.50.png}
}
\end{wrapfigure}

\begin{samepage}

Much like the assembler, the compiler takes an ASCII program, converts it into tokens and parses it into an Abstract Syntax Tree (AST) representing the structure and order of operations of the program. This AST is converted into an internal representation (IR) designed to help easily locate potential optimisations in the source code (e.g. pre-calculating arithmetic or removing redundant code), these optimisations are made and the IR is converted into an intermediate assembly language due to the presence of high level optimisations such as labels and macro-instructions. Finally, this assembly code is inserted into the assembler and the produced machine code is stored as a file on the users computer. 

\end{samepage}

\bigskip

\subsection{Component Design}
\subsubsection{Instruction Set Architecture}

\subsubsubsection{Computer Architecture}
Objective 1.1 (implementing a RISC architecture) meant many tradeoffs had to be made between convenience and practicality, There is 64Kb of memory (65,526 memory locations) specified by 16-bit addresses, requiring a 16-bit Program Counter (PC). The clock speed for the CPU runs at 600Hz. Each instruction is 32-bits, meaning two processor cycles are required to fetch an instruction and store the high and low words in the Current Instruction Register (CIR). 

I decided on 32 16-bit general purpose registers, any two of which can be inputs to the ALU, which itself has two outputs - a 16 bit result which can be written to a register or memory depending on the instruction, and a 1 bit zero bit which is set when the result of the calculation is 0. 

There are a number of special purpose registers, which, by convention are dedicated an address in memory, one for the stack pointer (SP) and another for the sound timer (ST). There is no dedicated stack and is instead allocated a downwards growing section of memory. The stack stores register values and return addresses when calling functions, however since the instruction set contains no \texttt{call} or \texttt{ret} instructions, this must be performed manually by the programmer. Each cycle the sound timer is non-zero, the computer produces a sound and decrements ST, allowing the length of the noise to be specified.

\needspace{100pt}

\subsubsubsection{Arithmetic and Logic Unit}

\begin{wrapfigure}[10]{r}{6.5cm}
    \shadowbox{
        \includegraphics[width=6cm]{ALU schema.png}
    }
\end{wrapfigure}

The core of any instruction set is the Arithmetic Logic Unit (ALU) so I began by designing an interface for that. I decided on 2 16-bit inputs to the ALU, which can either take register values, or for an Ri-type instruction, the value of the 16-bit immediate field. There are 4 ALU control bits into the ALU, whcih dictate the operation to be performed. The first two control bits negate their respective inputs, and the second two determine the arithmetic or logical operation to perform. Combinations of these ALU control bits can produce a variety of different operations, all of which are detailed in the table below. (\cite{EOCS})

\bigskip

\begin{center}
    \includegraphics[width=8cm]{Screenshot 2024-08-29 at 19.34.16.png}
\end{center}

\subsubsubsection{Assembly Language}

I decided to use a MIPS instruction set architecture, minimising the number of instructions supported by the processor. I settled on 2 instruction types: R-type instructions (performing ALU operations on register values), I-type instructions (operations involving both registers and immediate fields - includes jump and branch instructions) from these two types, the following instruction set can be constructed, demonstrated with a program to multiple two numbers stored in memory. (\texttt{'[]'} are used to indicate a memory address):

\begin{lstlisting}[language=C]
R-type: add, sub, and, or, nor, slt, sll, slr
I-type: addi, andi, ori, lw, sw, bge, bne
J-type: jmp, jr, jal

// multiply the numbers in memory address 0xb000 and 0xb001, and write the answer to 0xb002
li r31, 0xb000 // pointer to the start of data memory

lw r1, 0($r31)
lw r2, 1($r31)

.loop
    // if r2 is 0, break
    li r0, 0
    beq r0, r2, [.store]

    add r1, r1, r1

    addi r2, r2, -1

    jmp [.loop]

.store
    sw r1, 2($r31)
\end{lstlisting}

\subsubsubsection{Machine Code Encoding}
\label{sec:MachineCodeEncoding}
Below is the breakdown of how R/I/J type instructions are represented in binary, broken down into their respective fields. All instructions have a 4-bit opcode which dictates the type of instruction (and consequentially which of 2 decoding types should be used when decoding the instruction). In the machine code, Ri, L and J type instructions are encoded with the same operands, however they are each interpreted differently by the processor. 

\begin{itemize}
    \item \texttt{00-xx}: R-type instructions \textit{to perform operations between two registers}
        \begin{itemize}
            \item \texttt{5-bits rs}: the first of two input registers to the ALU.
            \item \texttt{5-bits rt}: the second of two input registers to the ALU.
            \item \texttt{5-bits rd}: the register in  which to store the result of the operation.
            \item \texttt{4-bits func}: the control bits determining the ALU operation.
        \end{itemize}
    \item \texttt{01-xx}: Ri-type instructions \textit{(to perform operations between a register and immediate value)}
        \begin{itemize}
            \item \texttt{5-bits rs}: the first of two input registers to the ALU.
            \item \texttt{5-bits rt}: the second of two input registers to the ALU.
            \item \texttt{16-bits immediate}: the data used as the second ALU input.
        \end{itemize}
    \item \texttt{10-xx}: L-type instructions \textit{(to load/store words from memory)}
        \begin{itemize}
            \item \texttt{5-bits rs}: the register containing the base offset for calculating memory addresses.
            \item \texttt{5-bits rt}: the register to store/read data from/into memory.
            \item \texttt{16-bits immediate}: the offset from the base address for the calculating memory addresses.
        \end{itemize}
    \item \texttt{11-xx}: J-type instructions \textit{(to jump to an address in memory)}
        \begin{itemize}
            \item \texttt{5-bits rs}: the register containing the memory address to branch to in RAM.
            \item \texttt{5-bits rt}: the register to store the return address of the jump.
            \item \texttt{16-bits immediate}: the address to branch to in RAM
        \end{itemize}
\end{itemize}
 
\bigskip

\begin{center}
\includegraphics[width=14cm]{Screenshot 2024-08-29 at 22.34.18.png}
\end{center}

\bigskip

Next to each assembly instruction below is its machine code encoding, showing the relationship between the assembly language operands and the encoding fields. 

\begin{lstlisting}
addi $r5, $r0, 14      // 0110 00000 00101 00000000 00001110
lw $r4, 10($r30)       // 1000 11110 00100 00000000 00001010


slt $r0, $r1, $r2z     // 0000 00001 00010 00000 0111 
bne $r0, $r4, [0xb020] // 1101 00000 00100 10110000 00100000
jmp [0x8080]           // 1111 10000000 10000000
\end{lstlisting}

Since each instruction is 32-bits and contains two words, an instruction must be stored across two memory locations. Little-endian and Big-endian are different standards for storing data across multiple bytes, for little endian - the LSB of the instruction is stored in the first memory location and the MSB in the second, vise versa for big endian. The modern standard has become little-endian encoding, and is what I will use in this project. Hence, the instruction \texttt{0110110000000101 0000000000001110} would be stored in memory as:

\begin{lstlisting}
    [0]: 0000000000001110
    [1]: 0110110000000101 
\end{lstlisting}

\subsubsubsection{High Level Language}
Following on from my client interview, the high level language should take inspiration from Go for its syntax and type system, although perhaps with a greater focus on pointers - due to the additional flexibility they open up for the programmer. I decided on the \texttt{func} keyword to define functions, and \texttt{->} to indicate the return type of a function. I also decided to mandate the data type in a variable declaration (specified by the \texttt{var} keyword, to aid parsing). The syntax for pointers is similar to that of C or C++, pointers are created with the \texttt{*} symbol and dereferenced with \texttt{\&}. Arrays and strings are null terminated and can be indexted using square bracket syntax (which is interchangable with \texttt{\&(array + index)} when working with memory addresses and pointers).

\begin{lstlisting}
func println(str: char*) {
    // copy character bytes from string to memory addresss 0x8000+ 
    // either &() or [] syntax can be used to write to or read from a pointers address
    for (var ptr: i16 = 0; str[ptr] != 0; ptr++) {
        var addr: u16* = 0x8000; 
        &(addr + ptr) = str[ptr];
    }
}

func main() -> int {
    var x: i16 = 10;
    var y: i16 = 5;
    var z: i16;

    if (x > y) {
        z = 1;
    } else {
        z = 0;
    }

    // strings are null terminated
    var str: char* = "Hello, World!"
    println(str);
}
\end{lstlisting}

\subsection{Virtual Machine}
The virtual machine should act as an interpreter, initialising a virtual processor - with an interface mimicking the registers, memory, and clock of the hardware descriptor. It needs to load a binary program cartridge into an array of bytes represening the computers memory, and steps through it word by word, decoding each instruction encountered and carrying out the subsequent operations accordingly. The data structure representing the processor will look as follows:

\begin{lstlisting}
STRUCT lion
    SIGNED WORD[65535] memory
    SIGNED WORD[32] registers
    UNSIGNED WORD pc
    UNSIGNED WORD elapsed_cycles;
ENDSTRUCT
\end{lstlisting}

The processor has $2^{16}$ different 16-bit memory locations, both this and the 32 general-purpose registers can be  represented by an array of signed words. The program counter must remain unsigned however, as its sole purpose is to point to locations in memory. The elapsed cycles field will be used to keep track of the number of cycles each instruction takes to execute - and to tell the emulator how long to wait for to maintain correct clock timing.

\subsubsection{Algorithms}

\subsubsubsection{Fetch-Execute Cycle}
The virtual machines primary directive is to simulate the fetch-decode execute cycle of the processor. This involves fetching an instruction from memory, incrementing the program counter, and then depending on the opcode of the fetched instruction, executing logic to handle that instruction. The \texttt{elapsed\_cycles} field on the struct can be used to keep a record of the time the program should wait after each instruction to maintain consistancy with the clock speed of the processor. This will be set depending on the instruction according to the processor architecture, e.g. a jump instruction may only take 2 cycles to execute, wheras a branch on equals would take 4. The basic structure for the fetch-execute cycle procedure should look as follows:

\begin{lstlisting}
PROCEDURE FETCH_EXECUTE()
    READ BINARY CARTRIDGE INTO MEMORY ARRAY

    WHILE PROCESSOR IS RUNNING
        IF CPU CYCLES BACKLOG IS EMPTY
            FETCH INSTRUCTION 
            INCREMENT PC

            MATCH ON INSTRUCTION OPCODE 
                R: 
                    MAP func FIELD TO OPERATION 
                    FETCH REGISTER VALUES 
                    PERFORM OPERATION ON REGISTERS 
                    STORE RESULT IN REGISTER 
                    SET CPU CYCLES BACKLOG 
                LW:
                    FETCH REGISTER VALUE
                    SUM ADDRESS AND REGISTER
                    SET ADDRESS TO RESULT
                    FETCH VALUE FROM MEMORY LOCATION
                    STORE THE RESULT IN REGISTER
                    SET CPU CYCLES BACKLOG
                JAL:
                    STORE PC VALUE IN REGISTER
                    SET PC TO IMMEDIATE FIELD
                    SET CPU CYCLES BACKLOG
        ELSE 
            DECREASE CPU CYCLES BACKLOG

        WAIT CLOCK CYCLE
ENDPROCEDURE
\end{lstlisting}

First the words from \texttt{memory[pc]} and \texttt{memory[pc+1]} must be fetched and stored together in a 32-bit current instruction register. From here, the 4-bit opcode should be extracted and used to determine how the instruction is executed. Each instruction will take multiple clock cycles to execute, and the field \texttt{elapsed\_cycles} on the \texttt{lion struct} is used to keep track of this, after each instruction is executed, the processor should wait for \texttt{elapsed\_cycles} clock cycles before proceeding to the next. 

Since each instruction is 32-bits, instructions span 2 memory locations and are stored according to the little endian convention, where the least significant byte is stored in the first location. Below is the pseudocode for fetch-execute cycle algorithm: 

\begin{lstlisting}
PROCEDURE FETCH_EXECUTE 
    READ_FILE_INTO_MEOMRY()

    WHILE (NOT hlt) DO 
        // only begin the next instruction cycle once the previous has finished executing
        IF (elapsed_cycles == 0) THEN 
            // LSB stored before the MSB in memory
            // binary shift the MSB to fit the LSB
            WORD cir = (memory[pc+1] << 16) | memory[pc]
            pc += 2

            EXECUTE_INSTRUCTION(cir)
        ELSE 
            elapsed_cycles -= 1
        ENDIF
z`

        WAIT 1/CLOCK_SPEED
    ENDWHILE
ENDPROCEDURE

PROCEDURE EXECUTE_INSTRUCTION(u32 instruction)
    NIBBLE opcode = instruction & 0xF000 
    MATCH (opcode) 
        CASE 0x0000: 
            UNSIGNED BYTE rs = instruction[4:9]
            UNSIGNED BYTE rt = instruction[10:15]
            UNSIGNED BYTE rd = instruction[16:21]
            UNSIGNED BYTE func = instruction[22:26]

            SIGNED WORD OP() = HASHMAP[func] 
            registers[rd] = OP(rs, rt)
            elapsed_cycles = 4
            BREAK

        CASE 0100: // lw rt, i16($rs)
            SIGNED WORD offset = instruction[16:]
            UNSIGNED BYTE rs = instruction[4:9]
            UNSIGNED BYTE rt = instruction[10:15]

            UNSIGNED WORD address = offset + registers[rs]
            registers[rt] = memory[address]
            elapsed_cycles = 4
            BREAK;

        CASE 0x1111: // jal i16
            UNSIGNED BYTE rt = instruction[10:15]
            registers[rt] = pc
            pc = instruction[16:]
            elapsed_cycles = 3
            BREAK
    END
END
\end{lstlisting}

\subsubsubsection{Reading a Binary File}
The virtual machine needs to be able to load a binary program from a file on the users computer into the processors memory array. This should default to the first address in memory since the pc (program counter) should point to the first instruction and the computer lacks a bootloader. The algorithm should open the file on the users computer from the filename provided in the CLI arguments, and reads it through two bytes at a time, loading each word into the processors memory array - until the end of the file has been reached.

\begin{lstlisting}
PROCEDURE READ_FILE_INTO_MEMORY() 
    IF (LEN(ARGS) <= 1) THEN 
        THROW "Missing filename argument"
    ENDIF 

    // get cartridge filename from cli arguments
    STRING filename = ARGS[1] 

    // open the file and read in the first word (2 bytes)
    FILE file = OPEN(filename)

    // since the file size is in bytes, and iterating through words, SIZE(file)/2
    FOR (WORD addr = 0; addr < SIZE(file)/2; addr++) DO
        memory[addr] = READ_WORD(file)
    ENDFOR
ENDPROCEDURE
\end{lstlisting}


\subsection{Assembler}
The assembler takes in an assembly program as a text file, and outputs the corresponding binary machine code. It consists of three parts: a lexical analysis of the text that produces an array of textual elements (e.g. NUMBER, LABEL, COLON); a parser that combines these program tokens into a sequence of instructions; and finally a compiler to synthesise machine code instructions. The broad pipeline for the assembler should look like the following:

\begin{lstlisting}
PROCEDURE assemble() 
    READ TEXT FILE INTO STRING

    // COMPILE PROGRAM STRING INTO A TOKEN ARRAY
    TOKEN LIST = lex()     

    // COMBINE TOKENS INTO A SERIES OF INSTRUCTIONS
    INSTRUCTION LIST = parse()   

    // CONVERT INSTRUCTIONS INTO MACHINE CODE
    MACHINE CODE = compile() 

    WRITE MACHINE CODE TO FILE
ENDPROCEDURE
\end{lstlisting}

\subsubsection{Data Structures}

\subsubsubsection{Token}
I need a token data structure to represent the program elements in the lexical stage of the assembler, it needs to be addressed in a uniform manner regardless of the type, and since different tokens can take different arguments - this logic needs to be split into an enum. The token struct should contain the position of the token (both start and end position since tokens can vary in length), and this can be used to generate error messages that point to the particular location in the file in which the error occured. 

\begin{lstlisting}
STRUCT TOKEN 
    TOKEN_TYPE t_type
    UNSIGNED INTEGER s_pos 
    UNSIGNED INTEGER e_pos 
ENDSTRUCT 

ENUM TOKEN_TYPE
    NUMBER(i16),
    LABEL(String),
    REGISTER(String),
    LPAREN,
    RPAREN,
    LSQUARE,
    RSQUARE
    DOLLAR,
    COMMA,
    DOT,
    ADD 
    BNE, 
    JAL
ENDENUM
\end{lstlisting}

\subsubsubsection{SyntaxError}
One of the objectives for this project (Objective 2.6) was the production of relevant error messages - pointing out the position in source code in which they occur. I decided on the format below for syntax errors. 

\begin{lstlisting}
SyntaxError: invalid character '%', line: 2
lw $r0, 10(%r0)
           ^
\end{lstlisting}

For this: two things are required, the error message itself, and the position of the error (start and end position). The program string itself instead of being included in the error can be passed as a parameter when displaying the message and the display method should be able to reference the line and character(s) on which the error occured from this.

\begin{lstlisting}
STRUCT SYNTAX_ERROR
    STRING msg
    UNSIGNED INTEGER s_pos 
    UNSIGNED INTEGER e_pos
ENDSTRUCT
\end{lstlisting}

The algorithm to display syntax errors should convert the s\_pos and e\_pos variables into a line number and column tuple, (initially an index in the 1 dimensional program string). It should then identify the column of the line in which the error occurs and print a carrat under that character. Since s\_pos and e\_pos refer to indexes in the program string, the line number and column in which they occur must be determined: after iterating through the string line by line and decrementing the variables by the number of characters on each line, when the variables are less than the length of the subsequent line, the (line, col) position of the error has been found.

\begin{lstlisting}
PROCEDURE DISPLAY_SYNTAX_ERROR(program: STRING)
    SPLIT PROGRAM INTO LINES
    ITERATE THROUGH EACH LINE 
        DECREMENT s_pos and e_pos BY EACH LINE LENGTH 

        WHEN s_pos < THIS LINE LENGTH 
        SET (s_line, s_col) TUPLE

        WHEN e_pos < THIS LINE LENGTH
        SET (e_line, e_col) TUPLE

    PRINT msg, "line: ", line
    PRINT line 
    PRINT " " * (s_col - 1) // SPACE BEFORE COL AND ERR
        + "^" * (e_col - s_col + 1) // LENGTH OF ERR
ENDPROCEDURE
\end{lstlisting}

\subsubsection{Algorithms}
\subsubsubsection{Lexical Analysis}
The lexer needs to step through the program string one character at a time and produce a vector of tokens containing the elements of the program (e.g. numbers, brackets or commas) that the parser can easily iterate through. 

\begin{lstlisting}
PROCEDURE lex() 
    ITERATE THROUGH THE PROGRAM CHARACTER BY CHARACTER
    CREATE A VECTOR OF TOKENS
    
    MATCH CHARACTER 
        FOR A SINGLE CHARACTER TOKEN e.g. ('(', '$',',')
        APPEND THE TOKEN TO THE VECTOR

        FOR A MULTI CHARACTER TOKEN e.g. numbers
        CREATE A NUMBER STRING
        WHILE character IS A NUMBER 
            APPEND character TO NUMBER STRING 
        ENDWHILE 
        PARSE NUMBER STRING TO INTEGER 
        APPEND TOKEN TO THE VECTOR
    ENDMATCH

    APPEND EOF TOKEN TO VECTOR
ENDPROCEDURE
\end{lstlisting}

I will create a lexer struct to store the internal state of the lexer: e.g. the current character, read position, program string. The lexer should have functions to advance (eat) a character, and peek at the next character.

\begin{lstlisting}
STRUCT LEXER 
    STRING program
    UNSIGNED INTEGER pos 
    CHAR ch
ENDSTRUCT

PROCEDURE LEXER.eat() 
    IF SELF.pos < LEN(SELF.program) THEN 
        SELF.pos += 1 
        SELF.ch = SELF.program[SELF.pos]
    ELSE 
        SELF.ch = EOF 
    END
ENDPROCEDURE

FUNCTION LEXER.peek() RETURNS CHAR
    IF SELF.pos + 1 < LEN(SELF.program) THEN 
        RETURN SELF.program[SELF.pos + 1]
    ELSE 
        RETURN NONE
    ENDIF
ENDFUNCTION
\end{lstlisting}

The \texttt{lex()} method should use the above functions to iterate through each character in the program string, if it encounters a single character token e.g. '(', '\$', that should be appended to the token vector. Should it encounter a alphabetic character, it should parse an identifier and build up a string of all consecutive alphanumeric characters. This identifier should be matched against the mneumonics and return a token specifying the specific instruction e.g. LW, or ADD. Should it encounter a number, if the first two characters are '0b' or '0x' the following digits should be parsed as binary or hexidecimal respectively - else they should be interpreted as denary. 

\begin{lstlisting}
PROCEDURE lex() 
    TOKEN[] tokens

    WHILE SELF.ch != EOF DO 
        TOKEN tok_type;
        UNSIGNED INTEGER s_pos = SELF.pos
        MATCH SELF.ch 
            CASE '(':
                tok_type = LPAREN
            CASE ')':
                tok_type = RPAREN
            CASE ',':
                tok_type = COMMA
            //[...]

            CASE '.':
                IF SELF.peek() is ALPHABETIC:
                    tok_type = lex_identifier()
                ELSE:
                    tok_type = DOT

            case SELF.ch is NUMERIC:
                UNSIGNED INTEGER base = 10
                IF SELF.ch == '0' AND SELF.peek() == 'b':
                    // point to the first character of the
                    // digit string for lexing
                    SELF.eat()
                    SELF.eat()
                    base = 2
                IF SELF.ch == '0' AND SELF.peek() == 'x':
                    SELF.eat()
                    SELF.eat()
                    base = 16
                ENDIF
                
                digit_str = lex_number()
                tok_type = NUMBER(INT(digit_str, base)) 

            case SELF.ch is ALPHABETIC:
                STRING identifier = lex_identifier()
                IF identifier IN mneumonics:
                    tok_type = mneumonics[identifier]
                ELSE 
                    THROW "SyntaxError: unexpected mneumonic"

        tokens.PUSH(
            Token(tok_type, s_pos: s_pos, e_pos: SELF.pos)
        )
    ENDWHILE

    RETURN tokens
ENDPROCEDURE

PROCEDURE lex_identifier() RETURNS STRING
    STRING str = SELF.ch
    WHILE SELF.peek() is ALPHANUMERIC DO 
        SELF.eat()
        str += SELF.ch
    ENDWHILE 

    // pos will point to the last character of the string 
    // meaning the next character will be skipped when 
    // eat() is called 
    SELF.rewind()
    RETURN str
ENDPROCEDURE
\end{lstlisting}

\subsubsubsection{Parsing}
Once the program has been converted into a list of tokens, the next step is to parse these tokens into a series of structs representing the assembly language instructions. The program should iterate through the tokens, and once it encounters a mneumonic, parse the operands of the instruction into a struct representing the encoding type. The function should return a list of these instruction structs. There are two valid types of statements the parser could encounter, a label definiton or an instruction, and therefore an interface is required for these two types under a the branch "Statement".

There are two formats I'm considering for instructions, either each instruction struct is identical and contains a vector of variable length of operands. This has the advantage of making parsing easier, however compilation will be more tedious.

Else, each instruction has a mneumoic field, and then one of 2 format types: I or R depending on the instruction being parsed. Since the \texttt{format} field could hold either \texttt{R\_format} or \texttt{I\_format}, and rust doesn't support inheritance - an enum can be used instead. 

\begin{lstlisting}
TRAIT Statement

STRUCT Label : Statement 
    String label 
    UNSIGNED INTEGER s_pos 
    UNSIGNED INTEGER e_pos 
ENDSTRUCT

STRUCT Instruction : Statement
    Token mneumonic;
    InstructionFormat format

    UNSIGNED INTEGER s_pos, 
    UNSIGNED INTEGER e_pos,
ENDSTRUCT

ENUM InstructionFormat
    R(R_format),
    I(I_format),
ENDENUM
    
STRUCT R_format 
    Register rs
    Register rt
    Register rd
ENDSTRUCT

STRUCT I_format 
    Register rs
    Register rt
    Number immediate
ENDSTRUCT
\end{lstlisting}

When parsing, the parser should iterate through each token - depending on which mneumonic it encounters, it should pass a different format of operands, e.g. a \texttt{LW} instruction should take the form \texttt{lw $rt, imm($rs)} wheras a \texttt{jmp} instruction \texttt{jmp imm}. The \texttt{eat\_expect()} function helps to simplify the code - as it both advances a token and throws an error if the token it encountered was not the one expected.

\begin{lstlisting}
STRUCT Parser
    Token token
    Token[] tokens
    UNSIGNED INTEGER pos
ENDSTRUCT

FUNCTION Parser.eat_expect(Token token) RETURNS Token | Error 
    self.eat() 
    IF self.token != token THEN
        THROW "Unexpected token encountered" 
    ELSE 
        RETURN self.token 
    ENDIF 
ENDFUNCTION

PROCEDURE parse()
    WHILE self.token != EOF 
        instruction = MATCH self.TOKEN
            CASE LABEL:
                LABEL

            CASE ADD, SUB, AND [...]:
                parse_R_format()

            CASE ADDI, ANDI, BEQ, BNE [...]:
                parse_Ri_format()

            CASE LW, SW: 
                parse_L_format()

            CASE JR: 
                [...]

            CASE JAL:
                [...]
        ENDMATCH

        self.eat()
        IF self.token != NEWLINE OR self.token != EOF 
            THROW "expected newline"
        ENDIF 
    ENDWHILE
ENDPROCEDURE

// mneumonic rd rs rt 
FUNCTION parse_R_format() RETURNS Instruction 
    Token mneumonic = self.token

    Token rd = self.eat_expect(Register)
    self.eat_expect(COMMA)

    Token rs = self.eat_expect(Register)
    self.eat_expect(COMMA)

    Token rt = self.eat_expect(Register)

    RETURN Instruction(mneumonic, R_format(rs, rt, rd))
ENDFUNCTION

// addi $r0, $r1, 0xf0ba
FUNCTION parse_Ri_format() RETURNS Instruction 
    Token mneumonic = self.token

    Token rt = self.eat_expect(Register)
    self.eat_expect(COMMA)

    Token rs = self.eat_expect(Register)
    self.eat_expect(COMMA)

    Token immediate = self.eat_expect(Number)

    RETURN Instruction(mneumonic, I_format(rs, rt, immediate))
ENDFUNCTION

// lw $rt, 10($rs)
FUNCTION parse_L_format() RETURNS Instruction
    Token mneumonic = self.token

    Token rt = self.eat_expect(Register)
    self.eat_expect(COMMA)

    Token immediate = self.eat_expect(Number)

    self.eat_expect(LPAREN)
    Token rs = self.eat_expect(Register)
    self.eat_expect(RPAREN)

    RETURN Instruction(mneumonic, I_format(rs, rt, immediate))    
ENDFUNCTION
\end{lstlisting}

\subsubsubsection{Compilation}
The compiler has broadly the same structure as the assembler: the file is fed through a lexer and then a parser to build up an abstract syntax tree, and from there, a code generator. However whilst the lexer is broadly similar to the assemblers, the parser is much more complicated. It needs to consider the order of operations of statements and expressions, parse prefix, infix and postfix expressions - multi character tokens, and consider the context in which any of these are used. The parsing algorithm I will use for this is a Pratt parser: it provides each operation with a binding precedence (e.g. multiplication binds stronger than addition) and parses expressions recursively - stopping a particular branch when it encounters an operator with a lower binding preference.

In a pratt parser programs are broken up into statements and expressions: statements are terminated by a semicolon and tend to be inflexible in their usage (such as a function definition or variable declaration), unlike expressions which can be passed around and evaluated. The grammar for my language will look as follows:

\begin{lstlisting}
statement:      function 
                | declaration

function:       FUNC identifier LPAREN (binding COMMA)* binding? RPAREN ( -> type)? block

declaration:    VAR binding EQ expression SEMICOLON

binding:        identifier : type

block:          LBRACE ( declaration | expression SEMICOLON )* expression? RBRACE

expression:     call 
                | binary 
                | prefix 
                | postfix 
                | literal 
                | variable 
                | assign 
                | break 
                | return
                | while 
                | for 
                | if

literal:        int | char | bool
assign:         expression EQ expression 
infix:          expression BINOP expression 
postfix:        expression POSTOP
prefix:         PREOP expression
if:             IF LPAREN expression RPAREN block (ELSE block)? 
for:            FOR LPAREN ( declaration? expression? SEMICOLON expression? SEMICOLON) block
\end{lstlisting}

\begin{lstlisting}
ENUM STATEMENT
    CONST {
        binding: Binding, 
        expression: EXPRESSION
    }

    FUNCTION {
        identifier: Identifier,
        bindings: Binding,
        return: Type,
        body: Block
    }
ENDENUM

ENUM EXPRESSION 
    INFIX {
        op: BINOP 
        lhs: EXPRESSION,
        rhs: EXPRESSION,
    }

    ASSIGN {
        lhs: EXPRESSION,
        rhs: EXPRESSION
    }

    IF {
        condition: EXPRESSION,
        consequence: Block,
        alternative: Block?,
    }

    EMPTY // '()'
ENDENUM

STRUCT Type 
    type: TYPE
    size: int
ENDSTRUCT  

ENUM TYPE 
    INT,
    CHAR,
    VOID,
    BOOL,
    STRUCT(Vec<TYPE>),
    ARRAY(TYPE, int),
    PTR(TYPE),
ENDENUM

STRUCT Binding 
    type: Type 
    identifier: Identifier
ENDSTRUCT

STRUCT Block 
    statements: Vec<STATEMENT>
    expression: EXPRESSION 
ENDSTRUCT
\end{lstlisting}

When iterating through a the list of tokens, the first thing to parse would be a statement, be that a function definition or variable declaration.

\begin{lstlisting}
FUNCTION parse_statement() RETURNS STATEMENT
    SWITCH self.token 
        CASE Token::FUNC:
            parse_func();

        CASE Token::CONST:
            parse_const();

        DEFAULT:
            THROW "unexpected token encountered"
ENDFUNCTION

FUNCTION parse_func() RETURNS STATEMENT 
    self.eat()
    Identifier identifier = self.parse_identifier()

    self.eat_assert(LPAREN)
    Binding[] params = [];
    WHILE self.token != RPAREN AND self.token != COMMA 
        self.parse_binding()
    ENDWHILE
    self.eat_assert(RPAREN)
    
    Type type;
    IF self.eat(RARROW) THEN
        type = self.parse_type()
    ELSE
        type = VOID
    ENDIF

    Body body = self.parse_block()

    RETURN Statement::FUNCTION(
        identifier,
        parameters,
        type,
        body
    )
ENDFUNCTION

FUNCTION parse_const() RETURNS STATEMENT
    self.eat()
    Binding binding = self.parse_binding()
    self.eat_assert(EQ)

    // verify that the expression simplifies down into a single literal 
    // in the optimisations module
    EXPRESSION expr = self.parse_expr()
    self.eat_assert(SEMICOLON)

    RETURN Statement::CONST(
        binding, 
        expr
    )
ENDFUNCTION
\end{lstlisting}

\begin{wrapfigure}[24]{r}{6.5cm}
    \shadowbox{
        \includegraphics[width=6cm]{Pratt Parser-2.png}
    }
\end{wrapfigure}

The \texttt{parse\_expression()} function will contain the meat of the compiler, it should first determine whether the token is a keyword (IF, FOR, WHILE): and if so, parse that accordingly. Otherwise, it should invoke the \texttt{parse\_pratt\_expression()} function. 

A pratt parser works by assigning each operation a binding preference, (e.g. multiplication binds with a higher binding power than addition, which itself binds with a higher power than logical operations like '=='). It recursively parses the right hand side of an infix operation (two operands seperated by an operator), whilst the binding power of the current operation is greater that than its parent. 

Prefix and postfix operations can also be supported in a pratt parser by verifying the token when parsing the literal on the left hand side of the equation. For example, when parsing \texttt{-a + b}, usually the parser would look for a literal token (a varaible name or number) and throw an error if it does not exist. However another condition to run \texttt{parse\_pratt\_expression()} once more, after it ecnounters a prefix operation with a higher binding preference and storing that expression as the left hand side allows brackets and prefix operations to be supported.

\begin{lstlisting}
FUNCTION parse_pratt_expression(INT precedence)
    EXPRESSION lhs
    SWITCH self.token 
        CASE NUMBER:
            lhs = NUMBER(number)
        CASE RPAREN:
            lhs  = parse_pratt_expression(0)
            eat_assert(RPAREN)
        CASE OP:
            EXPRESSION rhs = parse_pratt_expression(OP.prefix_binding_power)
            lhs = UNARY(OP, rhs)
    ENDSWITCH

    self.eat()
    INT cur_precedence = self.token.binding_power()
    WHILE self.token.is_operation() AND cur_precedence > precedence DO 
        self.eat()
        EXPRESSION rhs = parse_pratt_expression(self.token.binding_power + 1)
        cur_precedence = op.binding_power()
        lhs = INFIX(lhs, op, rhs)
    ENDWHILE

    RETURN lhs
ENDFUNCTION
\end{lstlisting}

\subsubsubsection{Optimizations \& Sentiment Analysis}
The third component of the compiler verifies the abstract syntax tree \& performs any optimisations it can. This involves checking variable types \& scopes, folding constants into a single literal, ensuring the presence of a \texttt{main()} function, and validating the left hand side of any assignment operations. 

Constant folding is the first operation the compiler will perform. For each expression in the abstract syntax tree, the program should recursively evaluate the left and the right hand side of the expression. Should an operation be performed on two literal nodes who's value is known at compile time (e.g. ints or chars) - the program should precalculate and store the result of the operation in the abstract syntax tree. 

\begin{lstlisting}
FUNCTION fold_constant(expr: EXPRESSION) RETURNS EXPRESSION
    IF expr is LITERAL 
        RETURN expr
    ELSE IF expr is INFIX 
        lhs = fold_constant(expr.lhs)
        rhs = fold_constant(expr.rhs)

        SWITCH op 
            CASE '+':
                return lhs + rhs
            CASE '*':
                return lhs * rhs
            // [...]
        ENDSWITCH 
    ENDIF
ENDFUNCTION
\end{lstlisting}

\subsubsubsection{Scoped Variables \& The Symbol Table}
To keep track of variables within the program, I will create a scope table. It should contain a list of \texttt{Scope} structs, addressible by a unique ScopeID for each scope. A scope needs to contain a hashmap of all the variables declared within that scope (stored with their respective types) and a reference to a potential parent scope. The data structures will look as follows:

\begin{lstlisting}
STRUCT SymbolTable
    symbols: HashMap<Identifier, SYMBOL> // store constants and function interfaces
    scopes: HashMap<ScopeID, Scope>
ENDSTRUCT

STRUCT Scope 
    variables: HashMap<Identifier, Variable>,
    parent: Option<Scope>
ENDSTRUCT

STRUCT Variable 
    type: TYPE,
ENDSTRUCT
\end{lstlisting}

Since code inside a particular scope can access variables declared within a larger, parent scope. Recursion can be used to help resolve a variable given an identifier and a current scope. 

\begin{lstlisting}
FUNCTION resolve_variable(scope: ScopeID, identifier: Identifier) RETURNS Option<Variable>
    current_scope = self.scopes.get(scope)

    IF current_scope.parent == NULL and !current_scope.contains(identifier) THEN
        RETURN None
    ELSE IF current_scope.contains(identifier)
        RETURN current_scope.get(identifier)
    ELSE 
        RETURN resolve_variable(current_scope.parent, identifier)
    ENDIF
ENDFUNCTION
\end{lstlisting}

\subsubsubsection{Type Checking}
Each expression in my language has a type, be that \texttt{int, void, char, bool} or a pointer to any of the previous. Assignment expressions have a void type. When validating the types of the AST, the program needs to recursively resolve the type of each expression, and ensure any contained expressions (e.g. lhs and rhs in an infix expression) are of compatible types. Further checks need to be performed to ensure that any conditionals resolve down too a boolean and a function is returning the correct type from within its function block. Depth first tree traversal can be used to ensure all nodes are being checked in the correct order.

\begin{lstlisting}
    
\end{lstlisting}

\subsubsubsection{Intermediate Representation \& Code Generation}
I will use a tuple based IR for this compiler as it most closely resembles an assembly language, making the code generation phase substantially simpler. Each tuple represents an instruction performed on a set of registers, labels or immediate values.

\begin{lstlisting}
ENUM Block
    ADD(Register, Register, Register) // add $rs + $rt and store in $rd
    ADDI(Register, Register, i16)     // add $rs and immediate value, store in $rd

    LABEL(String)
    JAL(Register, Label)           // jump to label and store return address in $rd
    JR(Regsiter)                   // jump to the address stored in a particular register 
    BEQ(Register, Register, Label) // jump to register if $rs == $rt

    MOV(Register, Register)     // set the contents of $rd to $rs
    LI(Register, i16)           // set the contents of $rd to an immediate value 
    LW(Register, Register, i16) // load the contents of memory in the address of immediate + $rs into $rd
ENDENUM
\end{lstlisting}

The program should iterate through the abstract syntax tree and build up a list of tuples representing each node in the program. For example should an if node be encountered, the produced IR may look as follows:

\begin{lstlisting}
if a < b {
    [...consequence]
} else {
    [...alternative]
}

SLT $t0, a, b
BEQ $t0, $zero, [.else]

[...consequence] 
jmp [.endif]

[.else]
[...alternative] 

[.endif]
\end{lstlisting}

A simple template can be used to generate the IR when compiling conditional statements, however when handling complex expressions you need to take care to store the result of any intermediate calculations. There are two methods for this: the first is a register based approach where the program keeps track of any free registers and assigns each to an intermediate step of the calculation. The second is a stack based approach where after each step the result of the calculation is pushed onto the stack and retrieved when it is required. I will use the registers \texttt{\$r0} and \texttt{\$r1} as a temporary registers to hold the results of calculations popped from teh stack before they have been operated upon.

\begin{lstlisting}
// the result of the expression should be stored in $r0
FUNCTION translate_expression(EXPRESSION expr) 
    SWITCH expr 
        CASE LITERAL:
            "LI $r0, [LITERAL]"

        CASE INFIX:
            translate_infix(expr)
    ENDSWITCH
ENDFUNCTION

FUNCTION translate_infix(EXPRESSION expr) 
    translate_expression(lhs)
    PUSH $r0

    translate_expression(rhs)
    POP $r1 // retrieve the value of the lhs from the stack

    SWITCH op 
        CASE ADD:
            ADD $r0, $r1, $r0

        CASE SUB:
            SUB $r0, $r1, $r0 

        [...]
    ENDSWITCH
ENDFUNCTION
\end{lstlisting}

For the compiler to reference locally scoped variables, they must be assigned a slot in the current stack frame. The function prologue needs to prepare the stack frame for any locally scoped variables and parameters and push the return address of the function onto the stack (so further function calls will not overwrite the original return stored in the register). This can be done by setting the base pointer (references the top of the stack) to the stack pointer and storing the old base pointer to retrieve when the function completes. This way when new elements are added to the stack, the compiler can assure they will not overwrite the previous functions scope. Once the function has terminated the function epiologue is responsible for returning the stack to the exact state it was before the function is called.

\begin{center}
    \shadowbox{
        \includegraphics[width=13cm]{Screenshot 2024-11-04 at 12.31.18.png}
    }
\end{center}

Once the stack has been prepared local variables can be stored internally as an offset from the base pointer of the current stack frame. e.g. in the example above, since the stack grows downwards, the address of variable \texttt{b} in the \texttt{main()} function could be calculated by adding 2 to the address in the base pointer.